
# ğŸ§  Estudo: Structured Streaming com Spark (Foco na DP-700)

Este guia aborda o funcionamento do Structured Streaming no Apache Spark, com foco em ingestÃ£o de dados em tempo real, especialmente utilizando **arquivos**, **Kafka** e **Azure Event Hubs**.

---

## ğŸ“Œ 1. O que Ã© Structured Streaming?

Structured Streaming Ã© uma **API de stream unificada** do Apache Spark que permite processar dados **em tempo real** como se fosse um DataFrame contÃ­nuo.

- Permite processar arquivos, eventos, mensagens, filas, etc.
- Escreve para sinks como Delta Lake, tabelas, bancos SQL, etc.
- Controla falhas e reprocessamentos com **checkpointing**

---

## ğŸ“ 2. Leitura contÃ­nua de arquivos (Data Lake)

### âœ… Leitura incremental de arquivos `.json`, `.csv`, `.parquet`:

```python
from pyspark.sql.types import *

schema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
])

df = spark.readStream     .schema(schema)     .option("maxFilesPerTrigger", 1)     .json("Files/data/")
```

> âš ï¸ Arquivos com extensÃ£o `.txt` funcionam desde que o conteÃºdo seja **JSON por linha**.

---

## ğŸ’¾ 3. Escrita contÃ­nua para Delta Lake

### âœ… Escrita com controle de estado:

```python
df.writeStream     .format("delta")     .outputMode("append")     .option("checkpointLocation", "Files/delta/checkpoint")     .start("Tables/iotdevicedata")
```

- `checkpointLocation`: controla quais arquivos jÃ¡ foram processados.
- Reiniciar o stream com o mesmo checkpoint evita duplicaÃ§Ã£o.
- Output mode `append` escreve apenas novos dados.

---

## ğŸ” 4. Leitura de fontes externas (ex: APIs, bancos)

**Structured Streaming nÃ£o lÃª diretamente de APIs ou bancos relacionais.**

### SoluÃ§Ã£o:

- Criar um processo (ex: com `foreachBatch`) que:
  - LÃª da fonte externa (ex: API)
  - Salva no Lake (JSON ou Delta)
- Structured Streaming lÃª os dados recÃ©m-salvos

---

## ğŸ›°ï¸ 5. Leitura com Kafka ou Event Hubs

### ğŸ”· Kafka

```python
df = spark.readStream     .format("kafka")     .option("kafka.bootstrap.servers", "localhost:9092")     .option("subscribe", "iot-topic")     .load()
```

### ğŸ”· Azure Event Hubs (via Kafka interface)

```python
ehConf = {
  "kafka.bootstrap.servers": "namespace.servicebus.windows.net:9093",
  "subscribe": "iot-topic",
  "kafka.security.protocol": "SASL_SSL",
  "kafka.sasl.mechanism": "PLAIN",
  "kafka.sasl.jaas.config": 'org.apache.kafka.common.security.plain.PlainLoginModule required username="$ConnectionString" password="<connection-string>";'
}

df = spark.readStream     .format("kafka")     .options(**ehConf)     .load()
```

### ğŸ§ª ConversÃ£o de dados:

```python
from pyspark.sql.functions import from_json

parsed = df.selectExpr("CAST(value AS STRING)")     .select(from_json("value", "device STRING, status STRING").alias("data"))     .select("data.*")
```

---

## ğŸ“¤ 6. Escrita para Delta Table (sink)

```python
parsed.writeStream     .format("delta")     .outputMode("append")     .option("checkpointLocation", "/Files/checkpoint/iot")     .start("/Tables/delta/iot_data")
```

---

## ğŸ§  7. Controle de Offset e Checkpoint

- Spark armazena o progresso com **offsets** (Kafka/EventHub) ou **arquivos lidos** (arquivos).
- Checkpoints sÃ£o obrigatÃ³rios para evitar duplicaÃ§Ã£o em falhas/reinÃ­cios.

---

## ğŸ“Š 8. Output Modes

| Modo        | DescriÃ§Ã£o |
|-------------|-----------|
| `append`    | Adiciona apenas novos dados |
| `complete`  | Reescreve o resultado completo (usado com agregaÃ§Ãµes) |
| `update`    | Atualiza apenas linhas modificadas |

---

## ğŸ“š 9. DiferenÃ§as: Kafka vs Event Hubs

| Item | Kafka | Event Hubs |
|------|-------|------------|
| Tipo | Open source | Azure PaaS |
| GerÃªncia | VocÃª gerencia a infra | Azure gerencia |
| SeguranÃ§a | SSL, ACLs | SAS, RBAC, VNET |
| Conectividade | Apache Kafka API | CompatÃ­vel com Kafka |

---

## âš ï¸ 10. Cuidados e Boas PrÃ¡ticas

- Sempre usar `checkpointLocation`
- Use `withWatermark()` para dados com atraso
- Prefira esquema fixo (`from_json`) para desempenho
- Controle tamanho dos lotes com `.trigger(...)`
- Teste com `rate` ou `socket` em ambientes de dev

---

## ğŸ§ª Exemplo completo: Kafka â†’ Spark â†’ Delta

```python
from pyspark.sql.functions import from_json

schema = "device STRING, status STRING"

df = spark.readStream     .format("kafka")     .option("kafka.bootstrap.servers", "localhost:9092")     .option("subscribe", "iot")     .load()

parsed = df.selectExpr("CAST(value AS STRING) as json")     .select(from_json("json", schema).alias("data"))     .select("data.*")

parsed.writeStream     .format("delta")     .option("checkpointLocation", "/tmp/checkpoint")     .start("/Tables/delta/iot_stream")
```

---

## ğŸ§  RecomendaÃ§Ãµes para a DP-700

- Entenda as diferenÃ§as entre streaming de arquivos, Kafka e Event Hubs
- Saiba configurar checkpoint, trigger e output modes
- Estude `foreachBatch`, `trigger`, `withWatermark`, `window`
- Pratique agregaÃ§Ãµes e joins com dados em tempo real
- Teste em ambiente com Delta Lake (Fabric, Databricks, etc)

---

## ğŸ“ Recursos adicionais

- [Structured Streaming Guide (Databricks)](https://docs.databricks.com/structured-streaming/index.html)
- [Apache Spark Streaming + Kafka](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Event Hubs para Apache Kafka](https://learn.microsoft.com/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview)
